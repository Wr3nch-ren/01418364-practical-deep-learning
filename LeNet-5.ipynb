{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a CNN to classify images in the CIFAR-10 Dataset\n",
    "\n",
    "We will work with the CIFAR-10 Dataset.  This is a well-known dataset for image classification, which consists of 60000 32x32 color images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "The 10 classes are:\n",
    "\n",
    "<ol start=\"0\">\n",
    "<li> airplane\n",
    "<li>  automobile\n",
    "<li> bird\n",
    "<li>  cat\n",
    "<li> deer\n",
    "<li> dog\n",
    "<li>  frog\n",
    "<li>  horse\n",
    "<li>  ship\n",
    "<li>  truck\n",
    "</ol>\n",
    "\n",
    "For details about CIFAR-10 see:\n",
    "https://www.cs.toronto.edu/~kriz/cifar.html\n",
    "\n",
    "For a compilation of published performance results on CIFAR 10, see:\n",
    "http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html\n",
    "\n",
    "---\n",
    "\n",
    "### Building Convolutional Neural Nets\n",
    "\n",
    "In this exercise we will build and train our first convolutional neural networks.  In the first part, we walk through the different layers and how they are configured.  In the second part, you will build your own model, train it, and compare the performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import random, numpy as np\n",
    "import keras, tensorflow as tf\n",
    "\n",
    "from keras.datasets import mnist\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D, BatchNormalization\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def set_seed(seed_value=1234):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    tf.random.set_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', \n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "# The data, shuffled and split between train and test sets:\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((28, 28), dtype('uint8'))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Each image is a 28 x 28 x 1 numpy array\n",
    "x_train[444].shape, x_train[444].dtype "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's look at one of the images\n",
    "\n",
    "# image_no = 444\n",
    "# print(y_train[image_no], class_names[y_train[image_no][0]])\n",
    "# plt.imshow(x_train[image_no]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 1., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now instead of classes described by an integer between 0-9 we have a vector with a 1 in the (Pythonic) 9th position\n",
    "y_train[444]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As before, let's make everything float and scale\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras Layers for CNNs\n",
    "- Previously we built Neural Networks using primarily the Dense, Activation and Dropout Layers.\n",
    "\n",
    "- Here we will describe how to use some of the CNN-specific layers provided by Keras\n",
    "\n",
    "### Conv2D\n",
    "\n",
    "```python\n",
    "keras.layers.convolutional.Conv2D(filters, kernel_size, strides=(1, 1), padding='valid', data_format=None, dilation_rate=(1, 1), activation=None, use_bias=True, kernel_initializer='glorot_uniform', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, bias_constraint=None, **kwargs)\n",
    "```\n",
    "\n",
    "A few parameters explained:\n",
    "- `filters`: the number of filter used per location.  In other words, the depth of the output.\n",
    "- `kernel_size`: an (x,y) tuple giving the height and width of the kernel to be used\n",
    "- `strides`: and (x,y) tuple giving the stride in each dimension.  Default is `(1,1)`\n",
    "- `input_shape`: required only for the first layer\n",
    "\n",
    "Note, the size of the output will be determined by the kernel_size, strides\n",
    "\n",
    "### MaxPooling2D\n",
    "`keras.layers.pooling.MaxPooling2D(pool_size=(2, 2), strides=None, padding='valid', data_format=None)`\n",
    "\n",
    "- `pool_size`: the (x,y) size of the grid to be pooled.\n",
    "- `strides`: Assumed to be the `pool_size` unless otherwise specified\n",
    "\n",
    "### Flatten\n",
    "Turns its input into a one-dimensional vector (per instance).  Usually used when transitioning between convolutional layers and fully connected layers.\n",
    "\n",
    "---\n",
    "\n",
    "## First CNN\n",
    "Below we will build our first CNN.  For demonstration purposes (so that it will train quickly) it is not very deep and has relatively few parameters.  We use strides of 2 in the first two convolutional layers which quickly reduces the dimensions of the output.  After a MaxPooling layer, we flatten, and then have a single fully connected layer before our final classification layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_seed(12345)\n",
    "\n",
    "# # Let's build a CNN using Keras' Sequential capabilities\n",
    "# model_1 = Sequential()\n",
    "\n",
    "\n",
    "# ## 5x5 convolution with 2x2 stride and 32 filters\n",
    "# model_1.add(Conv2D(32, (5, 5), strides=(2, 2), padding='same',\n",
    "#                  input_shape=x_train.shape[1:], activation='relu'))\n",
    "\n",
    "# ## Another 5x5 convolution with 2x2 stride and 32 filters\n",
    "# model_1.add(Conv2D(32, (5, 5), strides=(2, 2), activation='relu'))\n",
    "\n",
    "# ## 2x2 max pooling reduces to 3 x 3 x 32\n",
    "# model_1.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# ## Flatten turns 3x3x32 into 288x1\n",
    "# model_1.add(Flatten())\n",
    "# model_1.add(Dense(512, activation='relu'))\n",
    "# model_1.add(Dropout(0.5))\n",
    "# model_1.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "# model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have 181K parameters, even though this is a \"small\" model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m opt \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mRMSprop(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0005\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Let's train the model using RMSprop\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[43mmodel_1\u001b[49m\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m      8\u001b[0m                 optimizer\u001b[38;5;241m=\u001b[39mopt,\n\u001b[0;32m      9\u001b[0m                 metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     11\u001b[0m history_1 \u001b[38;5;241m=\u001b[39m model_1\u001b[38;5;241m.\u001b[39mfit(x_train, y_train,\n\u001b[0;32m     12\u001b[0m                         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[0;32m     13\u001b[0m                         epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[0;32m     14\u001b[0m                         validation_data\u001b[38;5;241m=\u001b[39m(x_test, y_test),\n\u001b[0;32m     15\u001b[0m                         shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_1' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.RMSprop(learning_rate=0.0005)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model_1.compile(loss='categorical_crossentropy',\n",
    "                optimizer=opt,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "history_1 = model_1.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=10,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 5), dpi=80)\n",
    "ax = fig.add_subplot(1, 2, 1)\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax.plot(history_1.history['loss'])\n",
    "ax.plot(history_1.history['val_loss'])\n",
    "ax.set_title('Loss')\n",
    "ax.set_ylabel('loss')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.legend(['train', 'val'], loc='upper right')\n",
    "\n",
    "ax = fig.add_subplot(1, 2, 2)\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax.plot(history_1.history['accuracy'])\n",
    "ax.plot(history_1.history['val_accuracy'])\n",
    "ax.set_title('Accuracy')\n",
    "ax.set_ylabel('accuracy')\n",
    "ax.set_xlabel('epoch')\n",
    "ax.legend(['train', 'val'], loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Loss: {:.6f}, Accuracy: {:.6f}'.format(*model_1.evaluate(x_train, y_train, verbose=0)))\n",
    "print(' Test Loss: {:.6f}, Accuracy: {:.6f}'.format(*model_1.evaluate(x_test, y_test, verbose=0)))\n",
    "print()\n",
    "\n",
    "y_pred = model_1.predict(x_test, verbose=0)\n",
    "\n",
    "y_test_1d = np.argmax(y_test, axis=1)\n",
    "y_pred_1d = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "cm = confusion_matrix(y_true=y_test_1d, y_pred=y_pred_1d).T\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams.update({'font.size': 12})\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "disp.plot(ax=ax, cmap=plt.cm.Blues)\n",
    "plt.xlabel('Actuals', fontsize=14)\n",
    "plt.ylabel('Predictions', fontsize=14)\n",
    "plt.xticks(fontsize=12, rotation=45)\n",
    "plt.yticks(fontsize=12)    \n",
    "plt.title('Confusion Matrix from model_1', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print(classification_report(y_true=y_test_1d, y_pred=y_pred_1d, target_names=class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "Accuracy &= \\frac{TP + TN}{TP + TN + FP + FN} \\\\\n",
    "Precision &= \\frac{TP}{TP + FP} \\\\\n",
    "Recall &= \\frac{TP}{TP + FN} \\\\\n",
    "F1-Score &= \\frac{2}{\\frac{1}{Precision} + \\frac{1}{Recall}} = 2 \\cdot \\frac{Precision \\cdot Recall}{Precision + Recall}\n",
    "\\end{align}\n",
    "$\n",
    "\n",
    "where \n",
    "$TP$: True Positives, $TN$: True Negatives, $FP$: False Positives, and $FN$: False Negatives\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### <span style=\"color:blue\">แบบฝึกปฏิบัติ</span>\n",
    "จากโครงสร้างของโมเดลเดิม:\n",
    "\n",
    "Conv -> Conv -> MaxPool -> (Flatten) -> Dense -> Final Classification\n",
    "\n",
    "(with appropriate activation functions and dropouts)\n",
    "\n",
    "1. ให้สร้างโมเดลใหม่ที่มีความซับซ้อนมากขึ้นโดยสามารถใช้รูปแบบโครงสร้างดังนี้:\n",
    "   - Conv -> Conv -> MaxPool -> Conv -> Conv -> MaxPool -> (Flatten) -> Dense -> Final Classification\n",
    "\n",
    "   - ให้ใช้ stride เป็น 1 สำหรับทุกเลเยอร์ convolutional.\n",
    "\n",
    "2. เทรนโมเดลใหม่โดยใช้เพียง 10 epochs (เท่ากับการเทรนโมเดลเดิม) และอธิบายสิ่งที่สังเกตได้ในเรื่องของเวลา และค่า loss และ accuracy ที่วัดได้จากทั้งชุดข้อมูลเทรนและเทส\n",
    "\n",
    "3. จากผลการทดสอบโมเดลใหม่ด้วยชุดข้อมูลเทส ให้สร้างกราฟ Confusion Matrix และแสดงรายงาน classification report\n",
    "\n",
    "4. หาภาพที่โมเดลใหม่ทำนายผิดมาอย่างน้อย 1 ภาพ โดยแสดงภาพให้ปรากฏ แสดงข้อความว่าภาพนั้นเป็นภาพของคลาสอะไร และโมเดลทำนายผิดเป็นคลาสอะไร"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinapat Fong-on 6410406541\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_14 (Conv2D)          (None, 28, 28, 6)         156       \n",
      "                                                                 \n",
      " average_pooling2d_8 (Averag  (None, 14, 14, 6)        0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 10, 10, 16)        2416      \n",
      "                                                                 \n",
      " average_pooling2d_9 (Averag  (None, 5, 5, 16)         0         \n",
      " ePooling2D)                                                     \n",
      "                                                                 \n",
      " flatten_7 (Flatten)         (None, 400)               0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 120)               48120     \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 84)                10164     \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 10)                850       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 61,706\n",
      "Trainable params: 61,706\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Using LeNet-5 Architecture\n",
    "\n",
    "model_2 = Sequential()\n",
    "\n",
    "model_2.add(Conv2D(filters=6, kernel_size=(5, 5), strides=(1, 1), activation='relu', input_shape=(32,32,1)))\n",
    "model_2.add(AveragePooling2D())\n",
    "\n",
    "model_2.add(Conv2D(filters=16, kernel_size=(5, 5), strides=(1, 1), activation='relu'))\n",
    "model_2.add(AveragePooling2D())\n",
    "\n",
    "model_2.add(Flatten())\n",
    "\n",
    "# model_2.add(Dense(units=400, activation='relu'))\n",
    "\n",
    "model_2.add(Dense(units=120, activation='relu'))\n",
    "\n",
    "model_2.add(Dense(units=84, activation='relu'))\n",
    "\n",
    "model_2.add(Dense(units=10, activation = 'softmax'))\n",
    "\n",
    "print(\"Chinapat Fong-on 6410406541\")\n",
    "model_2.summary()\n",
    "print(\"Chinapat Fong-on 6410406541\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_env",
   "language": "python",
   "name": "dl_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
